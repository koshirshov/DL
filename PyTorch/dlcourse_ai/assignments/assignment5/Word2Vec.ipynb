{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 5.1 - Word2Vec\n",
    "\n",
    "В этом задании мы натренируем свои word vectors на очень небольшом датасете.\n",
    "Мы будем использовать самую простую версию word2vec, без negative sampling и других оптимизаций.\n",
    "\n",
    "Перед запуском нужно запустить скрипт `download_data.sh` чтобы скачать данные.\n",
    "\n",
    "Датасет и модель очень небольшие, поэтому это задание можно выполнить и без GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll use Principal Component Analysis (PCA) to visualize word vectors,\n",
    "# so make sure you install dependencies from requirements.txt!\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 19538\n",
      "lawyer ['stalking', 'cop', 'grasps', 'investigating']\n",
      "electra ['disturbed', 'post-adolescent']\n",
      "yasujiro ['unchecked', 'heartache', 'ozu']\n",
      "imax ['splashed', 'immense']\n",
      "dud [\"'70s\", 'hills']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class StanfordTreeBank:\n",
    "    '''\n",
    "    Wrapper for accessing Stanford Tree Bank Dataset\n",
    "    https://nlp.stanford.edu/sentiment/treebank.html\n",
    "    \n",
    "    Parses dataset, gives each token and index and provides lookups\n",
    "    from string token to index and back\n",
    "    \n",
    "    Allows to generate random context with sampling strategy described in\n",
    "    word2vec paper:\n",
    "    https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.index_by_token = {}\n",
    "        self.token_by_index = []\n",
    "\n",
    "        self.sentences = []\n",
    "\n",
    "        self.token_freq = {}\n",
    "        \n",
    "        self.token_reject_by_index = None\n",
    "\n",
    "    def load_dataset(self, folder):\n",
    "        filename = os.path.join(folder, \"datasetSentences.txt\")\n",
    "\n",
    "        with open(filename, \"r\", encoding=\"latin1\") as f:\n",
    "            l = f.readline() # skip the first line\n",
    "            \n",
    "            for l in f:\n",
    "                splitted_line = l.strip().split()\n",
    "                words = [w.lower() for w in splitted_line[1:]] # First one is a number\n",
    "                    \n",
    "                self.sentences.append(words)\n",
    "                for word in words:\n",
    "                    if word in self.token_freq:\n",
    "                        self.token_freq[word] +=1 \n",
    "                    else:\n",
    "                        index = len(self.token_by_index)\n",
    "                        self.token_freq[word] = 1\n",
    "                        self.index_by_token[word] = index\n",
    "                        self.token_by_index.append(word)\n",
    "        self.compute_token_prob()\n",
    "                        \n",
    "    def compute_token_prob(self):\n",
    "        words_count = np.array([self.token_freq[token] for token in self.token_by_index])\n",
    "        words_freq = words_count / np.sum(words_count)\n",
    "        \n",
    "        # Following sampling strategy from word2vec paper:\n",
    "        # https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "        self.token_reject_by_index = 1- np.sqrt(1e-5/words_freq)\n",
    "    \n",
    "    def check_reject(self, word):\n",
    "        return np.random.rand() > self.token_reject_by_index[self.index_by_token[word]]\n",
    "        \n",
    "    def get_random_context(self, context_length=5):\n",
    "        \"\"\"\n",
    "        Returns tuple of center word and list of context words\n",
    "        \"\"\"\n",
    "        sentence_sampled = []\n",
    "        while len(sentence_sampled) <= 2:\n",
    "            sentence_index = np.random.randint(len(self.sentences)) \n",
    "            sentence = self.sentences[sentence_index]\n",
    "            sentence_sampled = [word for word in sentence if self.check_reject(word)]\n",
    "    \n",
    "        center_word_index = np.random.randint(len(sentence_sampled))\n",
    "        \n",
    "        words_before = sentence_sampled[max(center_word_index - context_length//2,0):center_word_index]\n",
    "        words_after = sentence_sampled[center_word_index+1: center_word_index+1+context_length//2]\n",
    "        \n",
    "        return sentence_sampled[center_word_index], words_before+words_after\n",
    "    \n",
    "    def num_tokens(self):\n",
    "        return len(self.token_by_index)\n",
    "        \n",
    "data = StanfordTreeBank()\n",
    "data.load_dataset(\"./stanfordSentimentTreebank/\")\n",
    "\n",
    "print(\"Num tokens:\", data.num_tokens())\n",
    "for i in range(5):\n",
    "    center_word, other_words = data.get_random_context(5)\n",
    "    print(center_word, other_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Имплеменируем PyTorch-style Dataset для Word2Vec\n",
    "\n",
    "Этот Dataset должен сгенерировать много случайных контекстов и превратить их в сэмплы для тренировки.\n",
    "\n",
    "Напоминаем, что word2vec модель получает на вход One-hot вектор слова и тренирует простую сеть для предсказания на его основе соседних слов.\n",
    "Из набора слово-контекст создается N сэмплов (где N - количество слов в контексте):\n",
    "\n",
    "Например:\n",
    "\n",
    "Слово: `orders` и контекст: `['love', 'nicest', 'to', '50-year']` создадут 4 сэмпла:\n",
    "- input: `orders`, target: `love`\n",
    "- input: `orders`, target: `nicest`\n",
    "- input: `orders`, target: `to`\n",
    "- input: `orders`, target: `50-year`\n",
    "\n",
    "Все слова на входе и на выходе закодированы через one-hot encoding, с размером вектора равным количеству токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample - input: tensor([0., 0., 0.,  ..., 0., 0., 0.]), target: 2505\n"
     ]
    }
   ],
   "source": [
    "class Word2VecPlain(Dataset):\n",
    "    '''\n",
    "    PyTorch Dataset for plain Word2Vec.\n",
    "    Accepts StanfordTreebank as data and is able to generate dataset based on\n",
    "    a number of random contexts\n",
    "    '''\n",
    "    def __init__(self, data, num_contexts=30000, folder=\"./stanfordSentimentTreebank/\"):\n",
    "        '''\n",
    "        Initializes Word2VecPlain, but doesn't generate the samples yet\n",
    "        (for that, use generate_dataset)\n",
    "        Arguments:\n",
    "        data - StanfordTreebank instance\n",
    "        num_contexts - number of random contexts to use when generating a dataset\n",
    "        '''\n",
    "        # TODO: Implement what you need for other methods!\n",
    "        \n",
    "        self.data = data\n",
    "        if not isinstance(data, StanfordTreeBank):\n",
    "            raise Exception(\"type of data can be StanfordTreeBank\")\n",
    "        self.num_contexts = num_contexts\n",
    "        self.num_tokens = data.num_tokens()\n",
    "        self.folder = folder\n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        '''\n",
    "        Generates dataset samples from random contexts\n",
    "        Note: there will be more samples than contexts because every context\n",
    "        can generate more than one sample\n",
    "        '''\n",
    "        # TODO: Implement generating the dataset\n",
    "        # You should sample num_contexts contexts from the data and turn them into samples\n",
    "        # Note you will have several samples from one context\n",
    "        self.data.load_dataset(self.folder)\n",
    "        random_contexts = [self.data.get_random_context() for _ in range(self.num_contexts)]\n",
    "        self.dataset = [(random_context[0], word) for random_context in random_contexts for word in random_context[1]]\n",
    "\n",
    "                \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns total number of samples\n",
    "        '''\n",
    "        # TODO: Return the number of samples\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns i-th sample\n",
    "        \n",
    "        Return values:\n",
    "        input_vector - torch.Tensor with one-hot representation of the input vector\n",
    "        output_index - index of the target word (not torch.Tensor!)\n",
    "        '''\n",
    "        # TODO: Generate tuple of 2 return arguments for i-th sample   \n",
    "        input_word, target = self.dataset[index]\n",
    "        \n",
    "        input_vector = torch.zeros(self.num_tokens)\n",
    "        input_vector[self.data.index_by_token[input_word]] = 1\n",
    "        output_index = self.data.index_by_token[target]\n",
    "        \n",
    "        return input_vector, output_index\n",
    "\n",
    "dataset = Word2VecPlain(data, 10)\n",
    "dataset.generate_dataset()\n",
    "input_vector, target = dataset[3]\n",
    "print(\"Sample - input: %s, target: %s\" % (input_vector, int(target))) # target should be able to convert to int\n",
    "assert isinstance(input_vector, torch.Tensor)\n",
    "assert torch.sum(input_vector) == 1.0\n",
    "assert input_vector.shape[0] == data.num_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаем модель и тренируем ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=19538, out_features=10, bias=False)\n",
       "  (1): Linear(in_features=10, out_features=19538, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the usual PyTorch structures\n",
    "dataset = Word2VecPlain(data, 30000)\n",
    "dataset.generate_dataset()\n",
    "\n",
    "# We'll be training very small word vectors!\n",
    "wordvec_dim = 10\n",
    "\n",
    "# We can use a standard sequential model for this\n",
    "nn_model = nn.Sequential(\n",
    "            nn.Linear(dataset.num_tokens, wordvec_dim, bias=False),\n",
    "            nn.Linear(wordvec_dim, dataset.num_tokens, bias=False), \n",
    "         )\n",
    "nn_model.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_vectors(nn_model):\n",
    "    '''\n",
    "    Extracts word vectors from the model\n",
    "    \n",
    "    Returns:\n",
    "    input_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
    "    output_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
    "    '''\n",
    "    # TODO: Implement extracting word vectors from param weights\n",
    "    # return tuple of input vectors and output vectos \n",
    "    # Hint: you can access weights as Tensors through nn.Linear class attributes\n",
    "    input_vectors = nn_model[0].weight.data.t().clone()\n",
    "    output_vectors = nn_model[1].weight.data.clone()\n",
    "    return input_vectors, output_vectors\n",
    "\n",
    "untrained_input_vectors, untrained_output_vectors = extract_word_vectors(nn_model)\n",
    "assert untrained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "assert untrained_output_vectors.shape == (data.num_tokens(), wordvec_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, train_loader, optimizer, scheduler, num_epochs):\n",
    "    '''\n",
    "    Trains plain word2vec using cross-entropy loss and regenerating dataset every epoch\n",
    "    \n",
    "    Returns:\n",
    "    loss_history, train_history\n",
    "    '''\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
    "    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        dataset.generate_dataset() # Regenerate dataset every epoch\n",
    "        \n",
    "        # TODO Implement training for this model\n",
    "        # Note we don't have any validation set here because our purpose is the word vectors,\n",
    "        # not the predictive performance of the model\n",
    "        #\n",
    "        # And don't forget to step the learing rate scheduler! \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            prediction = model(x)\n",
    "            loss_value = loss(prediction, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += float(loss_value)\n",
    "\n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        print(\"Epoch %i, Average loss: %f, Train accuracy: %f\" % (epoch, ave_loss, train_accuracy))\n",
    "        \n",
    "    return loss_history, train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну и наконец тренировка!\n",
    "\n",
    "Добейтесь значения ошибки меньше **8.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average loss: 9.887663, Train accuracy: 0.000012\n",
      "Epoch 1, Average loss: 9.887553, Train accuracy: 0.000048\n",
      "Epoch 2, Average loss: 9.887474, Train accuracy: 0.000097\n",
      "Epoch 3, Average loss: 9.887347, Train accuracy: 0.000145\n",
      "Epoch 4, Average loss: 9.887283, Train accuracy: 0.000097\n",
      "Epoch 5, Average loss: 9.887183, Train accuracy: 0.000120\n",
      "Epoch 6, Average loss: 9.887179, Train accuracy: 0.000193\n",
      "Epoch 7, Average loss: 9.887164, Train accuracy: 0.000206\n",
      "Epoch 8, Average loss: 9.887153, Train accuracy: 0.000242\n",
      "Epoch 9, Average loss: 9.887107, Train accuracy: 0.000133\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's train the model!\n",
    "\n",
    "# TODO: We use placeholder values for hyperparameters - you will need to find better values!\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.3)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=64)\n",
    "\n",
    "loss_history, train_history = train_model(nn_model, dataset, train_loader, optimizer, scheduler, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x208e464cec8>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training graphs\n",
    "plt.subplot(211)\n",
    "plt.plot(train_history)\n",
    "plt.subplot(212)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализируем вектора для разного вида слов до и после тренировки\n",
    "\n",
    "В случае успешной тренировки вы должны увидеть как вектора слов разных типов (например, знаков препинания, предлогов и остальных) разделяются семантически.\n",
    "\n",
    "Студенты - в качестве выполненного задания присылайте notebook с диаграммами!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_input_vectors, trained_output_vectors = extract_word_vectors(nn_model)\n",
    "assert trained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "assert trained_output_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "\n",
    "def visualize_vectors(input_vectors, output_vectors, title=''):\n",
    "    full_vectors = torch.cat((input_vectors, output_vectors), 0)\n",
    "    wordvec_embedding = PCA(n_components=2).fit_transform(full_vectors)\n",
    "\n",
    "    # Helpful words form CS244D example\n",
    "    # http://cs224d.stanford.edu/assignment1/index.html\n",
    "    visualize_words = {'green': [\"the\", \"a\", \"an\"], \n",
    "                      'blue': [\",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\"], \n",
    "                      'brown': [\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \n",
    "                              \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\"],\n",
    "                      'orange': [\"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\", \"stupid\"],\n",
    "                      'red': ['tell', 'told', 'said', 'say', 'says', 'tells', 'goes', 'go', 'went']\n",
    "                     }\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.suptitle(title)\n",
    "    for color, words in visualize_words.items():\n",
    "        points = np.array([wordvec_embedding[data.index_by_token[w]] for w in words])\n",
    "        for i, word in enumerate(words):\n",
    "            plt.text(points[i, 0], points[i, 1], word, color=color,horizontalalignment='center')\n",
    "        plt.scatter(points[:, 0], points[:, 1], c=color, alpha=0.3, s=0.5)\n",
    "\n",
    "visualize_vectors(untrained_input_vectors, untrained_output_vectors, \"Untrained word vectors\")\n",
    "visualize_vectors(trained_input_vectors, trained_output_vectors, \"Trained word vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
